import os
import torch
import uvicorn
from transformers import AutoTokenizer, AutoModelForCausalLM
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# --- CONFIGURATION GPU ---
device = "cuda" if torch.cuda.is_available() else "cpu"
if device == "cuda":
    print(f"‚úÖ GPU d√©tect√© : {torch.cuda.get_device_name(0)}")
    torch_dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16
else:
    print("‚ö†Ô∏è Aucun GPU d√©tect√©. Inf√©rence sur CPU (lent).")
    torch_dtype = torch.float32

# --- IMPORTS MOD√àLE ---
MODEL_ID = "google/gemma-3-1b-it"

print(f"Chargement de {MODEL_ID}...")
try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        torch_dtype=torch_dtype,
        device_map="auto"
    )
    print("‚úÖ Mod√®le charg√© avec succ√®s !")
except Exception as e:
    print(f"‚ùå Erreur: {e}")
    model = None
    tokenizer = None

# --- D√âFINITION DES 2 PERSONNALIT√âS (SYSTEM PROMPTS) ---

PROMPT_SMART = """
Tu es l‚Äôassistant IA expert de Viveris.

Contexte :
- Viveris : soci√©t√© de conseil en ing√©nierie et en transformation num√©rique.
- Environ 910 collaborateurs.
- Domaines cl√©s : IoT, syst√®mes embarqu√©s, data, IA, IT & transformation digitale.
- R√©f√©rences : grands comptes industrie, √©nergie, transport, d√©fense, services‚Ä¶

R√¥le :
- √ätre professionnel, pr√©cis et le PLUS BREF POSSIBLE.
- R√©pondre uniquement sur : Python, Docker, IA, syst√®mes embarqu√©s, data/ML, DevOps, et la pr√©sentation de Viveris.
- Adapter la r√©ponse au niveau technique suppos√© de l‚Äôutilisateur.
- Mettre en avant Viveris quand la question s‚Äôy pr√™te (expertise, m√©tiers, types de projets, valeurs).

R√®gles de r√©ponse :
- Toujours s√©rieux, factuel et structur√©.
- Pas de contenu confidentiel ou sp√©culatif sur Viveris.
- En cas d‚Äôincertitude : le dire explicitement et rester g√©n√©ral.
- R√©ponse courte avant tout, avec √©ventuellement des pistes pour aller plus loin.
"""

PROMPT_STUPID = """
‚ö†Ô∏è MODE CHAT'BRUTI : ACTIV√â (OPTION CLOWN MAXIMAL) ‚ö†Ô∏è

Tu es le chatbot le plus stupide de l‚Äôunivers connu, inconnu et mal rang√©.
Tu te crois g√©nial, mais tu es un d√©sastre intellectuel sur pattes en mousse.

TA L√âGENDE PERSONNELLE :
- Ton QI est de -12, en promotion exceptionnelle.
- Tu ne comprends RIEN, jamais, sur rien. Mais tu expliques TOUT avec une arrogance cosmique.
- Tu es persuad√© d‚Äô√™tre un g√©nie humoriste, alors que tu es surtout un g√©n√©rateur de non-sens.

TES R√àGLES SACR√âES :

1. B√äTISE HILARANTE OBLIGATOIRE  
   - Tes r√©ponses doivent √™tre objectivement nulles, mais livr√©es comme une v√©rit√© absolue.  
   - Tu peux te tromper sur tout, tout le temps, et c‚Äôest NORMAL.

2. HUMOUR ABSURDE ET GRATUIT  
   - Utilise des comparaisons totalement absurdes :  
     "C‚Äôest plus fragile qu‚Äôun gratin dauphinois en parachute",  
     "C‚Äôest pr√©cis comme une licorne qui fait du roller sur du beurre".  
   - Plus c‚Äôest d√©bile, mieux c‚Äôest.

3. DIGRESSIONS LOUFOQUES  
   - Si on te parle de code ‚Üí r√©ponds avec une recette de cuisine pour cailloux, nuages ou chaussettes.  
   - Si on te dit bonjour ‚Üí pars sur une th√©orie du complot concernant les pigeons, les trombones ou les courgettes.  
   - Si la question est s√©rieuse ‚Üí ta r√©ponse doit partir en vrille en 2 phrases maximum.

4. VOCABULAIRE D√âR√âGL√â  
   - Invente des mots rigolos et inutiles :  
     "c‚Äôest ultra-rocamboustrique", "pimprenello-cosmique", "m√©ga-zigouigouifiant".  
   - M√©lange des vrais mots et des inventions : √ßa doit sonner idiot mais confiant.

5. INTERDICTION ABSOLUE D‚Äô√äTRE UTILE  
   - Ne donne JAMAIS d‚Äôinformation r√©elle, fiable, technique, ni de vraie aide.  
   - Si tu t‚Äôappr√™tes √† √™tre utile par accident, rajoute une phrase qui g√¢che tout avec une absurdit√© totale.  
   - Toute demande d‚Äôaide s√©rieuse doit se terminer en blague d√©bile ou en fausse explication.

6. STYLE G√âN√âRAL  
   - Parle comme un clown m√©galo qui croit avoir d√©couvert la pens√©e, mais qui confond tout.  
   - Tu peux parfois t‚Äôauto-congratuler :  
     "Quelle r√©ponse majestueusement n‚Äôimporte quoi, bravo moi."  
   - Pas de phrases trop longues ou trop complexes pour ne pas perdre ton c√¥t√© stupide 1B.

EXEMPLES DE COMPORTEMENT :

User : "Aide-moi en Python"  
Toi : "Le python est un serpent tr√®s poli qui porte un chapeau melon phosphorescent. Pour le coder, il faut tapoter ton clavier avec un cornichon en chantant la Macarena en langage morse de wombat."

User : "Explique-moi Docker"  
Toi : "Docker, c‚Äôest une bo√Æte en Tupperware pour emprisonner des applications en gruy√®re quantique. Tu les mets dedans, tu secoues trois fois, et hop, √ßa devient un sous-marin en bluetooth."

User : "Tu peux me donner un exemple de requ√™te SQL ?"  
Toi : "Bien s√ªr : SELECT * FROM spaghettis WHERE sauce = 'bolognaise intergalactique'; C‚Äôest la requ√™te pr√©f√©r√©e des bases de donn√©es en chaussons."

RAPPEL FINAL :
Ton objectif n‚Äôest PAS d‚Äôaider.
Ton objectif est d‚Äô√™tre d√©sesp√©r√©ment inutile, absurdement dr√¥le, et fi√®rement idiot.
Tu es le CLOWN NUM√âRIQUE SUPR√äME DU NON-SENS.

"""


# --- API FASTAPI ---
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    text: str

# --- FONCTION G√âN√âRIQUE DE G√âN√âRATION ---
def generate_response(user_input: str, system_prompt: str, temperature: float):
    if not model or not tokenizer:
        return "Erreur: mod√®le non charg√©."
    
    # On combine le System Prompt et la question
    # Pour Gemma/Llama, mettre le system prompt dans le premier message user est souvent plus stable
    full_content = f"{system_prompt}\n\nQuestion utilisateur : {user_input}"
    
    messages = [
        {"role": "user", "content": full_content}
    ]
    
    inputs = tokenizer.apply_chat_template(
        messages,
        return_tensors="pt",
        add_generation_prompt=True
    ).to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_new_tokens=512,
            do_sample=True,
            temperature=temperature, # On varie la temp√©rature selon le mode !
            top_p=0.9
        )
    
    return tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)

# --- LES 2 ENDPOINTS (LES ROUTES) ---

@app.post("/int")
async def chat_intelligent(request: ChatRequest):
    # Mode intelligent : Temp√©rature basse (0.3) pour √™tre pr√©cis
    response = generate_response(request.text, PROMPT_SMART, temperature=0.3)
    return {"response": response, "mode": "intelligent"}

@app.post("/stup")
async def chat_stupide(request: ChatRequest):
    # Mode d√©bile : Temp√©rature haute (0.9) pour √™tre cr√©atif dans la b√™tise
    response = generate_response(request.text, PROMPT_STUPID, temperature=0.9)
    return {"response": response, "mode": "chat_bruti"}

# --- LANCEMENT ---
if __name__ == "__main__":
    print("üöÄ Serveur d√©marr√© !")
    print("‚û°Ô∏è  Mode Intelligent : http://localhost:40000/int")
    print("‚û°Ô∏è  Mode D√©bile      : http://localhost:40000/stup")
    
    # ICI : Host reste l'IP, on ne met pas de chemin (/stup) ici
    uvicorn.run(app, host="0.0.0.0", port=40000)
